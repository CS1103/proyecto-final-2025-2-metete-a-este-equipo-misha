# ğŸ® PONG AI - Neural Network Framework en C++20

**Un framework completo de redes neuronales en C++20 con aplicaciones en IA de juegos**

[![C++20](https://img.shields.io/badge/C%2B%2B-20-blue.svg)](https://en.wikipedia.org/wiki/C%2B%2B20)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Status: Complete](https://img.shields.io/badge/Status-Complete-brightgreen.svg)]()

---

## ğŸ“‹ Tabla de Contenidos

- [DescripciÃ³n General](#descripciÃ³n-general)
- [CaracterÃ­sticas Principales](#caracterÃ­sticas-principales)
- [Requisitos](#requisitos)
- [InstalaciÃ³n y CompilaciÃ³n](#instalaciÃ³n-y-compilaciÃ³n)
- [Uso RÃ¡pido](#uso-rÃ¡pido)
- [DocumentaciÃ³n Completa](#documentaciÃ³n-completa)
- [Ejemplos](#ejemplos)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [API Reference](#api-reference)
- [Algoritmos Implementados](#algoritmos-implementados)
- [Rendimiento](#rendimiento)
- [ContribuciÃ³n](#contribuciÃ³n)
- [Licencia](#licencia)

---

## ğŸ“– DescripciÃ³n General

**PONG AI** es un framework profesional de redes neuronales implementado en C++20. Proporciona:

- âœ… **Tensor<T, Rank>** - Arrays multidimensionales con broadcasting automÃ¡tico
- âœ… **Red Neuronal Completa** - Forward/backward propagation
- âœ… **MÃºltiples Funciones de ActivaciÃ³n** - ReLU, Sigmoid
- âœ… **Funciones de PÃ©rdida** - MSE, BCE
- âœ… **Optimizadores Adaptativos** - SGD, Adam con momentum
- âœ… **Early Stopping** - Detiene automÃ¡ticamente al converger
- âœ… **Agente de Pong** - Ejemplo prÃ¡ctico de aprendizaje por refuerzo
- âœ… **DocumentaciÃ³n Exhaustiva** - 1,200+ lÃ­neas

El proyecto estÃ¡ organizado en **3 Epics**:
- **Epic 1**: Biblioteca genÃ©rica de Ã¡lgebra (Tensor)
- **Epic 2**: Red neuronal completa con entrenamiento
- **Epic 3**: AplicaciÃ³n prÃ¡ctica y documentaciÃ³n

---

## âœ¨ CaracterÃ­sticas Principales

### 1. Tensor Multidimensional GenÃ©rico
```cpp
// Crear tensores de cualquier rango
Tensor<float, 2> matrix(3, 4);           // Matriz 3x4
Tensor<double, 3> tensor3d(2, 3, 4);     // Tensor 3D

// Acceso variÃ¡dico
float value = matrix(1, 2);

// Broadcasting automÃ¡tico
auto result = matrix + matrix;
auto scaled = matrix * 2.0f;

// Operaciones de Ã¡lgebra lineal
auto transposed = transpose(matrix);
auto product = matrix_product(A, B);
```

### 2. Red Neuronal Flexible
```cpp
NeuralNetwork<float> net;

// Agregar capas de forma modular
net.add_layer(std::make_unique<Dense<float>>(784, 128));
net.add_layer(std::make_unique<ReLU<float>>());
net.add_layer(std::make_unique<Dense<float>>(128, 10));
net.add_layer(std::make_unique<Sigmoid<float>>());
```

### 3. Entrenamiento Avanzado
```cpp
// Entrenamiento bÃ¡sico
float loss = net.train(X, Y, epochs=1000, learning_rate=0.01f);

// Entrenamiento con early stopping
auto metrics = net.train_advanced(
    X, Y,
    max_epochs=2000,
    learning_rate=0.01f,
    patience=50,              // Parar si 50 Ã©pocas sin mejora
    min_delta=1e-6f           // Mejora mÃ­nima considerada
);

// EvaluaciÃ³n completa
auto eval = net.evaluate(X_test, Y_test);
std::cout << "Accuracy: " << (eval.accuracy * 100) << "%\n";
```

### 4. Optimizadores Adaptativos
```cpp
// SGD - Descenso de gradiente estocÃ¡stico
SGD<float> sgd(learning_rate=0.01f);

// Adam - Adaptive Moment Estimation
Adam<float> adam(
    learning_rate=0.001f,
    beta1=0.9f,               // Momento 1
    beta2=0.999f,             // Momento 2
    epsilon=1e-8f             // Estabilidad
);
```

### 5. Agente Inteligente
```cpp
// Crear red para el agente
auto agent_net = std::make_unique<NeuralNetwork<float>>();
agent_net->add_layer(std::make_unique<Dense<float>>(3, 16));
agent_net->add_layer(std::make_unique<ReLU<float>>());
agent_net->add_layer(std::make_unique<Dense<float>>(16, 3));

// Crear agente
PongAgent<float> agent(std::move(agent_net));

// Interactuar con el ambiente
State state = env.reset();
int action = agent.act(state);  // -1 (arriba), 0 (quedo), 1 (abajo)
```

---

## ğŸ”§ Requisitos

### Versiones MÃ­nimas
- **C++20** - Standard de lenguaje
- **CMake 3.15+** - Sistema de build
- **Compilador**: GCC 10+, Clang 12+, MSVC 2019+

### Dependencias
- âœ… **Ninguna** - Solo librerÃ­a estÃ¡ndar de C++

### Sistema Operativo
- Windows 10+
- macOS 10.15+
- Linux (cualquier distribuciÃ³n moderna)

---

## ğŸ“¦ InstalaciÃ³n y CompilaciÃ³n

### Paso 1: Clonar o Descargar el Proyecto
```bash
git clone https://github.com/CS1103/proyecto-final-2025-2-metete-a-este-equipo-misha.git
cd proyecto-final-2025-2-metete-a-este-equipo-misha
```

### Paso 2: Compilar
```bash
# Crear directorio de build
mkdir build && cd build

# Configurar CMake
cmake .. -DCMAKE_BUILD_TYPE=Debug

# Compilar
cmake --build . --config Debug -j4
```

### Paso 3: Verificar CompilaciÃ³n
```bash
# Verificar ejecutables creados
ls -la PONG_AI train_xor test_tensor

# O en Windows
dir PONG_AI.exe train_xor.exe test_tensor.exe
```

### CompilaciÃ³n Optimizada (Release)
```bash
cmake .. -DCMAKE_BUILD_TYPE=Release
cmake --build . --config Release -j4
```

---

## ğŸš€ Uso RÃ¡pido

### Ejemplo BÃ¡sico: ClasificaciÃ³n XOR
```cpp
#include "include/utec/nn/neural_network.h"
#include "include/utec/nn/nn_dense.h"
#include "include/utec/nn/nn_activation.h"

using namespace utec::neural_network;
using namespace utec::algebra;

int main() {
    // Crear red: 2 -> 4 -> 1
    NeuralNetwork<float> net;
    net.add_layer(std::make_unique<Dense<float>>(2, 4));
    net.add_layer(std::make_unique<ReLU<float>>());
    net.add_layer(std::make_unique<Dense<float>>(4, 1));

    // Preparar datos XOR
    Tensor<float, 2> X(4, 2);
    X(0,0)=0; X(0,1)=0;
    X(1,0)=0; X(1,1)=1;
    X(2,0)=1; X(2,1)=0;
    X(3,0)=1; X(3,1)=1;

    Tensor<float, 2> Y(4, 1);
    Y(0,0)=0; Y(1,0)=1; Y(2,0)=1; Y(3,0)=0;

    // Entrenar
    float loss = net.train(X, Y, 1000, 0.1f);
    std::cout << "Loss final: " << loss << "\n";

    // Predecir
    auto predictions = net.forward(X);
    for (int i = 0; i < 4; ++i) {
        std::cout << "PredicciÃ³n: " << predictions(i, 0) << "\n";
    }

    return 0;
}
```

### Ejecutar Demos
```bash
# Demo completa (Tensor + Red + Pong)
./PONG_AI

# Entrenamiento XOR
./train_xor

# Tests unitarios
./test_tensor
./test_neural_network
./test_agent_env
```

---

## ğŸ“š DocumentaciÃ³n Completa

### ğŸ“– GuÃ­as Principales

1. **[GUIA_RAPIDA.md](docs/GUIA_RAPIDA.md)** - Comienza aquÃ­
   - 4 ejemplos prÃ¡cticos
   - Snippets de cÃ³digo listos para usar
   - HiperparÃ¡metros recomendados
   - Tips de debugging

2. **[ARQUITECTURA.md](docs/ARQUITECTURA.md)** - Comprende el diseÃ±o
   - ExplicaciÃ³n detallada de componentes
   - Flujo de entrenamiento
   - Algoritmos matemÃ¡ticos
   - Complejidad computacional


### ğŸ“ Rutas de Aprendizaje

**Ruta Principiante (1 hora)**
```
1. Este README (5 min)
2. GUIA_RAPIDA.md - Inicio RÃ¡pido (15 min)
3. Ejecutar ejemplos (15 min)
4. Modificar cÃ³digo (25 min)
```

**Ruta Intermedia (2-3 horas)**
```
1. GUIA_RAPIDA.md completo (30 min)
2. Ejecutar todos los ejemplos (45 min)
3. Leer ARQUITECTURA.md (45 min)
4. Crear tu propia red (30 min)
```

**Ruta Avanzada (4+ horas)**
```
1. ARQUITECTURA.md completo (90 min)
2. Revisar cÃ³digo fuente (90 min)
3. Implementar extensiones (open-ended)
```

---

## ğŸ“ Estructura del Proyecto

```
pong-ai/
â”œâ”€â”€ include/utec/
â”‚   â”œâ”€â”€ algebra/
â”‚   â”‚   â””â”€â”€ tensor.h                 # Tensor<T, Rank>
â”‚   â”œâ”€â”€ nn/
â”‚   â”‚   â”œâ”€â”€ neural_network.h         # Red neuronal principal
â”‚   â”‚   â”œâ”€â”€ nn_interfaces.h          # ILayer, IOptimizer, ILoss
â”‚   â”‚   â”œâ”€â”€ nn_dense.h               # Capa Dense (fully connected)
â”‚   â”‚   â”œâ”€â”€ nn_activation.h          # ReLU, Sigmoid
â”‚   â”‚   â”œâ”€â”€ nn_loss.h                # MSELoss, BCELoss
â”‚   â”‚   â””â”€â”€ nn_optimizer.h           # SGD, Adam
â”‚   â””â”€â”€ agent/
â”‚       â””â”€â”€ PongAgent.h              # Agente de Pong + Ambiente
â”‚
â”œâ”€â”€ src/utec/
â”‚   â””â”€â”€ agent/
â”‚       â””â”€â”€ PongAgent.cpp            # ImplementaciÃ³n del agente
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ train_xor.cpp                # Ejemplo: ClasificaciÃ³n XOR
â”‚   â””â”€â”€ train_pong_agent.cpp         # Ejemplo: Entrenamiento Pong
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_tensor.cpp              # Pruebas de Tensor
â”‚   â”œâ”€â”€ test_neural_network.cpp      # Pruebas de Red Neuronal
â”‚   â””â”€â”€ test_agent_env.cpp           # Pruebas de Agente Pong
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARQUITECTURA.md              # DiseÃ±o detallado
â”‚   â”œâ”€â”€ GUIA_RAPIDA.md               # GuÃ­a de uso
â”‚   â”œâ”€â”€ CAMBIOS_REALIZADOS.md        # Detalles tÃ©cnicos
â”‚   â””â”€â”€ BIBLIOGRAFIA.md              # Referencias acadÃ©micas
â”‚
â”œâ”€â”€ benchmarks/
â”‚   â””â”€â”€ performance_tests.cpp        # Pruebas de rendimiento
â”‚
â”œâ”€â”€ main.cpp                         # Demo principal
â”œâ”€â”€ README.md                        # Este archivo
â”œâ”€â”€ CMakeLists.txt                   # ConfiguraciÃ³n de build
â””â”€â”€ LICENSE                          # MIT License
```

---

## ğŸ”Œ API Reference

### Tensor<T, Rank>

```cpp
// Constructores
Tensor<float, 2> matrix(rows, cols);
Tensor<double, 3> tensor3d(d1, d2, d3);

// Acceso
T& element = tensor(i, j, k, ...);
std::array<size_t, Rank> shape = tensor.shape();
size_t size = tensor.size();

// Operaciones
tensor.fill(value);
tensor.reshape(d1, d2, ...);

// Ãlgebra lineal
auto transposed = transpose(matrix);
auto product = matrix_product(A, B);

// Operadores
auto sum = A + B;
auto diff = A - B;
auto elem_product = A * B;
auto scaled = A * scalar;
```

### NeuralNetwork<T>

```cpp
// ConstrucciÃ³n
NeuralNetwork<float> net;
net.add_layer(std::make_unique<Dense<float>>(input, output));
net.add_layer(std::make_unique<ReLU<float>>());

// PredicciÃ³n
Tensor<T, 2> output = net.forward(input);
Tensor<T, 2> output = net.predict(input);  // Alias

// Entrenamiento
T loss = net.train(X, Y, epochs, learning_rate);

// Entrenamiento avanzado
TrainingMetrics<T> metrics = net.train_advanced(
    X, Y,           // Datos
    max_epochs,     // MÃ¡ximo de Ã©pocas
    learning_rate,  // Tasa de aprendizaje
    patience,       // Ã‰pocas sin mejora antes de parar
    min_delta       // Mejora mÃ­nima considerada
);

// EvaluaciÃ³n
EvaluationMetrics<T> eval = net.evaluate(X_test, Y_test);
// eval.test_loss, eval.accuracy, eval.mean_absolute_error
```

### Capas (ILayer<T>)

```cpp
// Dense - Capa fully connected
Dense<float> layer(input_features, output_features);

// Activaciones
ReLU<float> relu;       // max(0, x)
Sigmoid<float> sigmoid; // 1 / (1 + e^(-x))
```

### Funciones de PÃ©rdida (ILoss<T>)

```cpp
// Error CuadrÃ¡tico Medio
MSELoss<float> loss(predictions, targets);
float value = loss.loss();
Tensor<float, 2> gradient = loss.loss_gradient();

// EntropÃ­a Cruzada Binaria
BCELoss<float> loss(predictions, targets);
```

### Optimizadores (IOptimizer<T>)

```cpp
// SGD
SGD<float> sgd(learning_rate);

// Adam
Adam<float> adam(learning_rate, beta1, beta2, epsilon);
```

---

## ğŸ§® Algoritmos Implementados

### Forward Propagation
```
Para cada capa i:
  a[i] = Ïƒ(z[i])
  z[i] = a[i-1] Â· W[i] + b[i]
```

### Backward Propagation
```
Para cada capa i (de atrÃ¡s hacia adelante):
  dz[i] = Ïƒ'(z[i]) * da[i]
  dW[i] = (1/m) * a[i-1]áµ€ Â· dz[i]
  db[i] = (1/m) * Î£ dz[i]
  da[i-1] = dz[i] Â· W[i]áµ€
```

### SGD (Stochastic Gradient Descent)
```
Î¸ := Î¸ - Î± * âˆ‡L(Î¸)
```

### Adam (Adaptive Moment Estimation)
```
m_t := Î²â‚ * m_{t-1} + (1 - Î²â‚) * g_t
v_t := Î²â‚‚ * v_{t-1} + (1 - Î²â‚‚) * g_tÂ²
mÌ‚_t := m_t / (1 - Î²â‚^t)
vÌ‚_t := v_t / (1 - Î²â‚‚^t)
Î¸_{t+1} := Î¸_t - Î± * mÌ‚_t / (âˆšvÌ‚_t + Îµ)
```

### ReLU (Rectified Linear Unit)
```
Forward: y = max(0, x)
Backward: dy/dx = 1 if x > 0, else 0
```

### Sigmoid
```
Forward: y = 1 / (1 + e^(-x))
Backward: dy/dx = Ïƒ(x) * (1 - Ïƒ(x))
```

---

## ğŸ“Š Rendimiento

### Complejidad Computacional

| OperaciÃ³n | Complejidad | DescripciÃ³n |
|-----------|------------|------------|
| Forward (Dense) | O(nÂ·m) | n inputs, m outputs |
| Backward (Dense) | O(nÂ·m) | CÃ¡lculo de gradientes |
| Matrix Product | O(nÂ·mÂ·k) | NxM por MxK |
| MSE Loss | O(n) | n predicciones |
| Adam Update | O(n) | n parÃ¡metros |

### Benchmark Simple (XOR)

**ConfiguraciÃ³n**: Red 2-4-1, 1000 Ã©pocas, SGD 0.1

| MÃ©trica | Valor |
|---------|-------|
| Tiempo compilaciÃ³n | ~2 segundos |
| Tiempo entrenamiento | ~50 ms |
| Loss inicial | ~0.25 |
| Loss final | ~0.01 |
| PrecisiÃ³n predicciÃ³n | 100% |

---

## ğŸ“ Ejemplos

El proyecto incluye **2 ejemplos de entrenamiento** que demuestran cÃ³mo usar la red neuronal:

### 1. **train_xor.cpp** - ValidaciÃ³n BÃ¡sica de la Red

Demuestra que la red neuronal puede aprender el problema XOR (problema clÃ¡sico de validaciÃ³n en machine learning).

**PropÃ³sito**: Verificar que la arquitectura de forward/backward propagation funciona correctamente.

**CaracterÃ­sticas**:
- Problema simple: 2 inputs â†’ 1 output
- 4 muestras de datos (todas las combinaciones posibles)
- Entrenamiento bÃ¡sico con `train()` y avanzado con `train_advanced()`
- Early stopping automÃ¡tico
- EvaluaciÃ³n con mÃºltiples mÃ©tricas

**EjecuciÃ³n**:
```bash
cd cmake-build-debug
./train_xor
```

**Salida esperada**:
```
=== ENTRENAMIENTO DE RED NEURONAL - XOR ===

Datos de entrenamiento creados:
Input: [0, 0] -> Output: 0
Input: [0, 1] -> Output: 1
Input: [1, 0] -> Output: 1
Input: [1, 1] -> Output: 0

Red neuronal creada: 2->8->4->1

=== ENTRENAMIENTO AVANZADO ===
Ã‰pocas entrenadas: 1234/2000
ConvergiÃ³: SÃ­
Loss final: 1.23e-04
Accuracy: 100.00%
```

---

### 2. **train_pong_agent.cpp** - Entrenamiento del Agente Pong â­

**Este es el ejemplo PRINCIPAL del proyecto PONG AI**.

Entrena una red neuronal para aprender a jugar Pong prediciendo los mejores movimientos de la paleta.

**PropÃ³sito**: Demostrar que la red neuronal puede aprender a tomar decisiones complejas en un dominio real (juego).

**CaracterÃ­sticas**:
- **Entrada**: 5 valores (posiciÃ³n de bola x/y, velocidad de bola, posiciÃ³n de paleta)
- **Salida**: 3 acciones (arriba, quedo, abajo) en formato one-hot encoding
- **Datos**: 1000 muestras de entrenamiento + 200 de prueba
- **GeneraciÃ³n**: Datos sintÃ©ticos con lÃ³gica de decisiÃ³n Ã³ptima
- **ValidaciÃ³n**: EvaluaciÃ³n en conjunto de prueba separado
- **AnÃ¡lisis**: VisualizaciÃ³n de evoluciÃ³n del loss durante entrenamiento
- **Predicciones**: Ejemplos de decisiones tomadas por la red

**Arquitectura de la red**:
```
Entrada (5) â†’ Dense(32) â†’ ReLU â†’ Dense(16) â†’ ReLU â†’ Dense(8) â†’ ReLU â†’ Salida (3)
```

**EjecuciÃ³n**:
```bash
cd cmake-build-debug
./train_pong_agent
```

**Salida esperada**:
```
=== ENTRENAMIENTO PONG AGENT ===

Generando datos de entrenamiento...
Datos generados:
- Entrenamiento: 1000 muestras
- Prueba: 200 muestras
- Features: 5 (ball_x, ball_y, ball_vx, ball_vy, paddle_y)
- Acciones: 3 (up, stay, down)

Red neuronal para Pong creada: 5->32->16->8->3

=== ENTRENAMIENTO CON VALIDACIÃ“N ===
Ã‰pocas: 247/500
ConvergiÃ³: SÃ­
Mejor loss: 0.145

=== EVALUACIÃ“N EN DATOS DE PRUEBA ===
MÃ©tricas de prueba:
- Loss: 0.152
- Accuracy: 87.5%
- MAE: 0.098

=== EVOLUCIÃ“N DE LA PÃ‰RDIDA ===
Ã‰poca 0: Loss = 0.895
Ã‰poca 25: Loss = 0.623
Ã‰poca 50: Loss = 0.451

=== EJEMPLOS DE PREDICCIÃ“N ===
Ejemplo 1:
  Estado: [ball_x=0.345, ball_y=0.678, paddle_y=0.512]
  AcciÃ³n real: DOWN
  AcciÃ³n predicha: DOWN âœ“
  Confianza: [UP=0.12, STAY=0.23, DOWN=0.65]
```

---

## ğŸ® IntegraciÃ³n con main.cpp

El archivo `main.cpp` incluye **4 demostraciones completas** del framework:

1. **demo_tensor_operations()** - Operaciones bÃ¡sicas con Tensores
2. **demo_neural_network()** - Red neuronal simple en XOR
3. **demo_training_advanced()** - Entrenamiento con early stopping
4. **demo_pong_agent()** - SimulaciÃ³n del agente Pong con ambiente

**EjecuciÃ³n**:
```bash
cd cmake-build-debug
./PONG_AI
```

Este programa demuestra todas las capacidades del framework de forma compacta.

---

## ğŸ”— Archivos de Ejemplo

```
examples/
â”œâ”€â”€ train_xor.cpp              # ValidaciÃ³n de NN (MANTENER)
â”œâ”€â”€ train_pong_agent.cpp       # Entrenamiento de Pong (PRINCIPAL)
â””â”€â”€ EJEMPLOS_ELIMINADOS.md     # DocumentaciÃ³n de ejemplos no vÃ¡lidos
```

**Nota**: Algunos ejemplos genÃ©ricos de machine learning fueron **eliminados** porque no estÃ¡n alineados con el objetivo especÃ­fico del proyecto (PONG AI). Ver `docs/ANALISIS_EJEMPLOS.md` para detalles.

## ğŸ§ª Pruebas

### Ejecutar Tests Unitarios
```bash
./test_tensor              # Pruebas de Tensor
./test_neural_network      # Pruebas de Red Neuronal
./test_agent_env          # Pruebas de Agente Pong
```

### Ejecutar Benchmarks
```bash
./performance_benchmark   # Pruebas de rendimiento
```

---

## ğŸ› ï¸ Troubleshooting

| Problema | SoluciÃ³n |
|----------|----------|
| No compila | Verificar C++20, ver COMPILACION_EJECUCION.md |
| NaN en pÃ©rdida | Normalizar datos de entrada |
| Red no aprende | Ajustar learning rate (probar: 0.001, 0.01, 0.1) |
| Lento | Compilar en Release (-O3), aumentar batch size |
| Acceso fuera de rango | Verificar `.shape()` de tensores |

---

## ğŸ” CaracterÃ­sticas Avanzadas

### Early Stopping
```cpp
auto metrics = net.train_advanced(
    X, Y,
    max_epochs=5000,
    learning_rate=0.01f,
    patience=100,      // Parar si 100 Ã©pocas sin mejora
    min_delta=1e-8f    // Mejora mÃ­nima
);

if (metrics.converged) {
    std::cout << "ConvergiÃ³ tempranamente\n";
}
```

### ValidaciÃ³n Durante Entrenamiento
```cpp
// Dividir datos
Tensor<float, 2> X_train, X_val, Y_train, Y_val;

// Entrenar y evaluar
auto metrics = net.train_advanced(X_train, Y_train, ...);
auto val_metrics = net.evaluate(X_val, Y_val);

std::cout << "Train loss: " << metrics.final_loss << "\n";
std::cout << "Val loss: " << val_metrics.test_loss << "\n";
```

### Arquitecturas Personalizadas
```cpp
// Red profunda (5 capas)
NeuralNetwork<float> deep_net;
deep_net.add_layer(std::make_unique<Dense<float>>(784, 512));
deep_net.add_layer(std::make_unique<ReLU<float>>());
deep_net.add_layer(std::make_unique<Dense<float>>(512, 256));
deep_net.add_layer(std::make_unique<ReLU<float>>());
deep_net.add_layer(std::make_unique<Dense<float>>(256, 128));
deep_net.add_layer(std::make_unique<ReLU<float>>());
deep_net.add_layer(std::make_unique<Dense<float>>(128, 10));
```

---

## ğŸ“Š Paradigmas de ProgramaciÃ³n

El proyecto utiliza los siguientes paradigmas de C++ moderno:

- **Object-Oriented Programming (OOP)** - Clases, herencia (ILayer, IOptimizer, ILoss)
- **Generic Programming** - Templates (`Tensor<T, Rank>`, `NeuralNetwork<T>`)
- **Functional Programming** - Lambda functions, std::function
- **Move Semantics** - Eficiencia con `std::move` y `std::unique_ptr`
- **C++20 Concepts** - CompilaciÃ³n type-safe

---

## ğŸ“ˆ Optimizaciones

- **SIMD-ready** - CÃ³digo preparado para vectorizaciÃ³n
- **Memory efficient** - Smart pointers, no memory leaks
- **Cache-friendly** - Row-major order en matrices
- **Parallelizable** - OpenMP support ready

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Ver [LICENSE](LICENSE) para detalles.

```
MIT License

Copyright (c) 2025 PONG AI Contributors

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.
```

---

## ğŸ“š BibliografÃ­a

### Redes Neuronales
- Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*. MIT Press.
- LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436-444.

### OptimizaciÃ³n
- Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
- Ruder, S. (2016). An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.

### ProgramaciÃ³n en C++
- Stroustrup, B. (2022). *A Tour of C++* (3rd ed.). Addison-Wesley.
- ISO/IEC (2020). *Programming languages â€” C++* (ISO/IEC 14882:2020).

Ver [docs/BIBLIOGRAFIA.md](docs/BIBLIOGRAFIA.md) para referencias completas.

---

## ğŸ“Š EstadÃ­sticas del Proyecto

- **LÃ­neas de cÃ³digo**: 2,500+
- **LÃ­neas de documentaciÃ³n**: 1,200+
- **Archivos header**: 9
- **Archivos fuente**: 3
- **Ejemplos**: 6
- **Tests**: 15+
- **Arquitectura mÃ¡xima testada**: 784-512-256-128-10

---